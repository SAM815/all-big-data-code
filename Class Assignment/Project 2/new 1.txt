=======
first thing we will do is learn to group the movies seen by a user
==================
this code works for the above:
import sys
from pyspark import SparkContext, SparkConf

if __name__ == "__main__": #checks if script being run as main program
    # Initialize Spark contex
		
		#set the application name to highest rated movie per user and set master url  to run spark in local mode
	
        conf = SparkConf().setAppName("HighestRatedMoviePerUser").setMaster("local")
		
		
		#creating a sparkcontext object
        sc = SparkContext(conf=conf)
		
		#load data into a textfile. textFile is a function provided by SparkContext to read text files into an RDD (Resilient Distributed Dataset).
        data = sc.textFile("input/input.txt") 

        #We split each line of the input data by comma , using the map() function
        users = data.map(lambda line: line.split(","))


        # then we create tuples of (user, movie) pairs. creates tuples from the list containing user and movie. may ask question Why a tuple?
        user_movie_pairs = users.map(lambda x: (x[0], x[1]))

        # Group all movies by userId
		#groupByKey() function. This function groups the values of the RDD by their keys. In this case, it groups movies by user ID, so each user will have a list of movies associated with them.
		
        movies_by_user = user_movie_pairs.groupByKey();

        #map each users to a list of movies they have watched
        formatted_output = movies_by_user.flatMap(lambda x: x[0] + " " + ", ".join(x[1]))
		
		
		
		#movies that users liked
		user_movie_rating_liked = user_movie_rating_pairs.groupByKey()

    # Save the formatted string to a folder named 'output'
        formatted_output.saveAsTextFile("/path/to/output")

    # Stop SparkContext
        sc.stop()
		
		
		=============================================================
		now learn to group the movies along if the users liked them or not
		#1 we found all the movies that user-xliked and user-y liked i.e 
		#all the movies that the users(all of them) liked
		
		================================================================
		
import sys
from pyspark import SparkContext, SparkConf

if __name__ == "__main__":
    # Initialize Spark context
    conf = SparkConf().setAppName("FilteredMoviesPerUser").setMaster("local")
    sc = SparkContext(conf=conf)
    
    # Load data into a text file
    data = sc.textFile("input/input.txt")
    
    # Split each line of the input data by comma
    users = data.map(lambda line: line.split(","))
    
    # Create tuples of (user, movie, rating) from input data
    user_movie_rating = users.map(lambda x: (x[0], (x[1], int(x[2]))))
    
    # Group movies by user ID
    movies_by_user = user_movie_rating.groupByKey()
    
    # Filter movies with ratings more than 2 for each user
    filtered_movies_by_user = movies_by_user.mapValues(lambda movies: [(movie, rating) for movie, rating in movies if rating > 2])
    
    # Format the output as (user, [(movie, rating), ...])
	# all movies that users liked
    formatted_output = filtered_movies_by_user.map(lambda x: (x[0], list(x[1])))
	
	#create a list of liked movies
	#list_of_liked_movies = formatted_output.flatMap(lambda x: [movie[0] for movie in x[1]]).distinct().collect()
    
	list_of_liked_movies = formatted_output.flatMap(lambda x: [(movie[0], 1) for movie in x[1]]) \
                                            .reduceByKey(lambda a, b: a) \
                                            .map(lambda x: x[0]) \
                                            .collect()
    
    
	#only user and movies which will be later used.
	only_user_and_movies = formatted_output.map(lambda x: (x[0], [movie[0] for movie in x[1]]))
	
	result_list = only_user_and_movies.collect()

    # Loop through the list and print each element
    for user, movies in result_list:
         print(f"User: {user}, Movies: {movies}")
	
	# Print the list of liked movies
    #print("List of liked movies:", list_of_liked_movies)
	
    # Save the formatted tuples to a folder named 'output'
    #formatted_output.saveAsTextFile("/path/to/filtered_output")
    
    # Stop SparkContext
    sc.stop()


(user_6,(movie_21,4), (movie_7,5), (movie_9,5))

(user_8,(movie_30,3), (movie_2,4), (movie_4,5))

(user_2,(movie_22,4), (movie_6,5), (movie_2,4))

the output comes like this: List of liked movies: ['movie_27', 'movie_1', 'movie_18', 'movie_7', 'movie_13', 'movie_10', 'movie_5', 'movie_22', 'movie_3', 'movie_6', 'movie_11', 'movie_23', 'movie_19', 'movie_12', 'movie_16', 'movie_26', 'movie_2', 'movie_21', 'movie_9', 'movie_15', 'movie_20', 'movie_14', 'movie_28', 'movie_30', 'movie_4']

==========================================================
now find all the movies that user has each user has not watched


==========================================================

import sys
from pyspark import SparkContext, SparkConf

if __name__ == "__main__":
    # Initialize Spark context
    conf = SparkConf().setAppName("FilteredMoviesPerUser").setMaster("local")
    sc = SparkContext(conf=conf)
    
    # Load data into a text file
    data = sc.textFile("input/input.txt")
    
    # Split each line of the input data by comma
    users = data.map(lambda line: line.split(","))
    
    # Create tuples of (user, movie, rating) from input data
    user_movie_rating = users.map(lambda x: (x[0], (x[1], int(x[2]))))
    
    # Group movies by user ID
    movies_by_user = user_movie_rating.groupByKey()
    
    # Filter movies with ratings more than 2 for each user
    filtered_movies_by_user = movies_by_user.mapValues(lambda movies: [(movie, rating) for movie, rating in movies if rating > 2])
    
    # Format the output as (user, [(movie, rating), ...])
    # all movies that users liked
    formatted_output = filtered_movies_by_user.map(lambda x: (x[0], list(x[1])))
    
    # Create a list of liked movies
    list_of_liked_movies = formatted_output.flatMap(lambda x: [(movie[0], 1) for movie in x[1]]) \
                                            .reduceByKey(lambda a, b: a) \
                                            .map(lambda x: x[0]) \
                                            .collect()
    
    # Only user and movies which will be later used.
    only_user_and_movies = formatted_output.map(lambda x: (x[0], [movie[0] for movie in x[1]]))
    
    # Compare the movies that the user has watched to the list of liked movies
    # and store the movies that the user has not watched
    recommended_movies_by_user = only_user_and_movies.mapValues(lambda movies: list(set(list_of_liked_movies) - set(movies)))
    
    # Collect the results
    result_list = recommended_movies_by_user.collect()

    # Loop through the list and print each element
    for user, recommended_movies in result_list:
        print(f"User: {user}, Recommended Movies: {recommended_movies}")
    
    # Stop SparkContext
    sc.stop()




===========
27,1, 24